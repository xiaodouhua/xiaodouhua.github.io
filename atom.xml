<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Dou&#39;s Blog</title>
  <subtitle>Just For Fun</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="xiaodouhua.github.io/"/>
  <updated>2017-07-04T15:08:30.540Z</updated>
  <id>xiaodouhua.github.io/</id>
  
  <author>
    <name>Dou</name>
    <email>doulongchao@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>一个简单的验证码识别</title>
    <link href="xiaodouhua.github.io/2017/07/04/%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB/"/>
    <id>xiaodouhua.github.io/2017/07/04/一个简单的验证码识别/</id>
    <published>2017-07-04T14:40:18.000Z</published>
    <updated>2017-07-04T15:08:30.540Z</updated>
    
    <content type="html"><![CDATA[<p>一个验证码识别实验，欢迎star。<a href="https://github.com/xiaodouhua/captcha_recognition" target="_blank" rel="external">https://github.com/xiaodouhua/captcha_recognition</a><br>实验用到的库主要是sklearn，建议下载anaconda即可</p>
<p>实验主要工作有以下几点：</p>
<ul>
<li>批量下载验证码</li>
<li>验证码的处理，包括：去噪音，灰度化，二值化，<em>切割</em></li>
<li>用svm+pca去训练数据</li>
</ul>
<p>各个文件介绍：</p>
<ul>
<li>googleocr.py: 一行代码，主要用了google的pytesseract开源识别库，在此数据集上不能识别，应为数据是空心的。如果是实心的话，效果猜测不错（前一个项目就是直接用的，基本100%）</li>
<li>scrapy.py: 爬取数据集</li>
<li>slice_pic.py: 图片处理，切割等</li>
<li>ml.py ：利用sklearn库进行数据特征提取，pca降维，svm分类，保存了模型</li>
<li>predict.py: 利用保存的预测<br>实验数据如下 ：<br><img src="https://raw.githubusercontent.com/xiaodouhua/captcha_recognition/master/0.jpg" alt="image">  </li>
</ul>
<p>共爬取了1000张，最后由于人工去标注的量太大，所以只用了分割后的910张去训练和测试准确率<br>训练数据为910*0.85，剩下的去测试，最后经过pca调参之后选取了降维到50个特征，切割后的单个图片的验证码识别率为85%~86%，所以验证码的识别率大概为0.85^4≈50%，效果一般，但是应该可以用了。</p>
<p>总结：</p>
<ul>
<li>切割的非常好，因为数据集的原因，观察观察再观察，发现数据切割只要固定切割就行了，不需要发现空白竖线什么的（反而不好发现），而且恰好是均等分割，后面特征处理非常好处理。 </li>
</ul>
<p><img src="https://raw.githubusercontent.com/xiaodouhua/captcha_recognition/master/3.jpg" alt="image"> </p>
<ul>
<li>训练结果在单个字符上也是不错的，在经过pca各种参数维度降维之后，选取了50。</li>
</ul>
<p>不足之处：</p>
<ul>
<li>噪音线条和数字字母的颜色太相近，很难去去噪，只去除了黑点和颜色比较深的部分，去除之后如下：</li>
</ul>
<p><img src="https://raw.githubusercontent.com/xiaodouhua/captcha_recognition/master/1.jpg" alt="image"><br><img src="https://raw.githubusercontent.com/xiaodouhua/captcha_recognition/master/2.jpg" alt="image"> </p>
<ul>
<li>识别只用了机器学习的算法，看网上的资料说神经网络应该效果不错的。还有水滴算法等</li>
<li>数据集还是太少，人工需要把每个切割后的图片放到对应文件里面，很费时。如果数据量大的话，算法的准确率应该会再上一台阶。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一个验证码识别实验，欢迎star。&lt;a href=&quot;https://github.com/xiaodouhua/captcha_recognition&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/xiaodouhua
    
    </summary>
    
      <category term="技术" scheme="xiaodouhua.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="算法" scheme="xiaodouhua.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="ML" scheme="xiaodouhua.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法6_决策树</title>
    <link href="xiaodouhua.github.io/2017/07/01/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%956_%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>xiaodouhua.github.io/2017/07/01/统计学习方法6_决策树/</id>
    <published>2017-07-01T13:54:58.244Z</published>
    <updated>2017-07-01T12:21:53.722Z</updated>
    
    <content type="html"><![CDATA[<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a><strong>决策树</strong></h1><p>决策树（decision tree）是一种基本的分类与回归方法，这里主要讨论用于分类的决策树。它<strong>可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布</strong>。其主要的有点是模型具有<strong>可读性，分类速度快</strong>，学习时利用训练数据，根据损失函数最小化的原则简历决策树模型。决策树的学习通常包括三个步骤：<strong>特征选择，决策树的生成和决策树的修剪</strong>。</p>
<h2 id="决策树模型与学习"><a href="#决策树模型与学习" class="headerlink" title="决策树模型与学习"></a>决策树模型与学习</h2><h3 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h3><p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部节点和叶节点，内部节点表示一个特征或属性，叶节点表示一个类。</p>
<p>分类的时候，从根节点开始，当前节点设为根节点，当前节点必定是一种特征，根据实例的该特征的取值，向下移动，直到到达叶节点，将实例分到叶节点对应的类中。</p>
<h3 id="决策树与if-then规则"><a href="#决策树与if-then规则" class="headerlink" title="决策树与if-then规则"></a>决策树与if-then规则</h3><p>决策树的属性结构其实对应着一个规则集合：由决策树的根节点到叶节点的每条路径构成的规则组成；路径上的内部特征对应着if条件，叶节点对应着then结论。决策树和规则集合是等效的，都具有一个重要的性质：<strong>互斥且完备</strong>。也就是说<strong>任何实例都被且仅被一条路径或规则覆盖</strong>。</p>
<h3 id="决策树与条件概率分布"><a href="#决策树与条件概率分布" class="headerlink" title="决策树与条件概率分布"></a>决策树与条件概率分布</h3><p>决策树还是给定特征条件下类的条件概率分布。该条件分布定义在特征空间的划分上，特征空间被花费为互不相交的单元，每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的每条路径对应于划分中的一个单元。给定实例的特征X，一定落入某个划分，决策树选取该划分里最大概率的类作为结果输出。如图：</p>
<p><img src="http://www.ppvke.com/Blog/wp-content/uploads/2015/11/31d167e80a09a46b488017e5dee1f303.jpg" alt="决策树与条件概率分布.png"></p>
<p>关于b图，将a图的基础上增加一个条件概率的维度P，代表在当前特征X的情况下，分类为+的后验概率。图中的方块有些地方完全没有，比如x2轴上[a2,1]这个区间，说明只要X落在这里，Y就一定是-的，同理对于[0,a1]和[0,a2]围起来的一定是+的。有些地方只有一半，比如x1轴上[a1,1]这个区间，说明决策树认为X落在这里，Y只有一半概率是+的，根据选择条件概率大的类别的原则，就认为Y是-的（因为不满足P(+)&gt;0.5)。</p>
<h3 id="决策树学习"><a href="#决策树学习" class="headerlink" title="决策树学习"></a>决策树学习</h3><p>决策树学习，假设给定训练数据集D={(x1,y1),(x2,y2),⋯,(xN,yN)}</p>
<p>其中，xi为输入实例（特征向量）yi为类标记，N为样本容量。学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。 </p>
<p>决策树学习本质上是从训练数据集中归纳出一组分类规则。</p>
<p>我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。另一个角度看，决策树学习是由训练数据集估计条件概率模型。我们选择的条件概率模型应该不仅对训练数据有很好的拟合，而且对未知数据有很好的预测。 </p>
<p>决策树学习用损失函数表示这一目标。如下所述，决策树学习的损失函数通常是正则化的极大似然函数。</p>
<p>决策树学习的策略是以损失函数为目标函数的最小化。</p>
<p>当损失函数确定以后，学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题。这样得到的决策树是次最优(sub-optimal)的。 </p>
<p>决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。</p>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a><strong>特征选择</strong></h2><h3 id="特征选择问题"><a href="#特征选择问题" class="headerlink" title="特征选择问题"></a><strong>特征选择问题</strong></h3><p><strong>特征选择在于选取对训练数据具有分类能力的特征。</strong>这样可以提高决策树学习的效率。<strong>如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。</strong>经验上扔掉这样的特征对决策树学习的精度影响不大。通常<strong>特征选择的准则</strong>是<strong>信息增益或信息增益比</strong></p>
<h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a><strong>信息增益</strong></h3><p>在信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为$P(X=x_i)=p<em>i,i=1,2,\cdots,n$，则随机变量X的熵定义为$H(X) = -\sum\limits</em>{i=1}^{n}{p_i\log{p_i}}$</p>
<p>通常上式中的对数以2为底或者以自然对数e为底，这时熵的单位分别称作比特（bit）或纳特（nat）。由定义可知，熵只依赖于X分布，而与X的取值无关.</p>
<p>熵越大，随机变量的不确定性就越大。从定义可以验证 $0 \le H(p)\le \log n$,当随机变量只取两个值，例如1,0时，即X的分布为 P(X=1)=p,P(X=0)=1−p,0≤p≤1熵为$H(p)=-p \log_2p-(1-p)\log_2(1-p)$</p>
<p>由此可知熵随p变化的曲线图：</p>
<p><a href="http://photo.blog.sina.com.cn/showpic.html#blogid=e561ac510102v9w2&amp;url=http://album.sina.com.cn/pic/004crqmJgy6N5VDcCf3ed" target="_blank" rel="external"><img src="http://s14.sinaimg.cn/small/004crqmJgy6N5VDcCf3ed&amp;690" alt="决策树（主要参考李航的统计学习方法）"></a></p>
<p>所以当p取0或1时，熵的值为1，变量X完全没有不确定性，当p取0.5时，变量X的不确定性最大。</p>
<p>条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性，定义为在X给定的条件下，Y的条件概率分布对X的数学期望：$H(Y|X) = \sum\limits_{i=1}^{n}{p_iH(Y|X=x_i)}$,其中，$p_i=P(X=x_i), i=1,2,…,n$</p>
<p>信息增益（information gain）:表示在得知特征X的条件下，类Y的不确定性减少的程度。定义为训练数据D的经验熵与给定特征X的条件下的数据D的经验条件熵的差值。</p>
<p>特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即<br>$g(D,A)=H(D)−H(D|A)$</p>
<p><strong>一般地，熵H(Y)与条件熵H(Y|X)之差称为互信息（mutual information）</strong>。决策树学习中的信息增益等价于训练数据集中的类与特征的互信息。<br><strong>决策树学习应用信息增益准则选择特征。</strong>信息增益大的特征具有更强的分类能力。<br>根据信息增益准则的<strong>特征选择方法</strong>是：对训练数据集（或子集）D,计算其每个特征的信息增益，并比较他们的大小，选择信息增益最大的特征。</p>
<h3 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a><strong>信息增益比</strong></h3><p>以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比（information gain ratio）可以对这一问题进行校正。这是特征选择的另一准则。 </p>
<p><strong>定义（信息增益比）：</strong>特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集D关于特征A的值的熵$H_A(D)$之比，即 :</p>
<p>$g_R(D,A) = \frac{g(D,A)}{H_A(D)}$</p>
<p>其中$H<em>A(D)=-\sum</em>{i=1}^n\frac{|D_i|}{|D|}\log_2 \frac{|D_i|}{|D|}$</p>
<h2 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a><strong>决策树的生成</strong></h2><h3 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a><strong>ID3算法</strong></h3><p>ID3算法（interative dichotomiser 3）的核心是在决策树各个结点上应用<strong>信息增益准则</strong>选择特征，<strong>递归</strong>地构建决策树。具体方法是：从根结点（root node）开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。ID3相当于用极大似然法进行概率模型的选择。 </p>
<p><strong>ID3算法只有树的生成，所以该算法生成的树容易产生过拟合。</strong></p>
<h3 id="C4-5的生成方法"><a href="#C4-5的生成方法" class="headerlink" title="C4.5的生成方法"></a><strong>C4.5的生成方法</strong></h3><p>C4.5算法与ID3算法相似，C4.5算法对ID3算法进行了改进。C4.5在生成的过程中，用信息增益比来选择特征。</p>
<h2 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a><strong>决策树的剪枝</strong></h2><p><strong>决策树生成算法递归地产生决策树，直到不能继续下去为止。这样产生的树容易出现过拟合现象。</strong>过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树，解决这一问题的方法是考虑决策树的复杂度，对已生成的决策树进行简化。<br><strong>在决策树学习中讲已生成的树进行简化的过程称为剪枝（pruning）。</strong><br>决策树的剪枝往往通过极小化决策树整体的损失函数（loss function）或代价函数（cost function）来实现。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;决策树&quot;&gt;&lt;a href=&quot;#决策树&quot; class=&quot;headerlink&quot; title=&quot;决策树&quot;&gt;&lt;/a&gt;&lt;strong&gt;决策树&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;决策树（decision tree）是一种基本的分类与回归方法，这里主要讨论用于分类的决策树。它&lt;
    
    </summary>
    
      <category term="读书" scheme="xiaodouhua.github.io/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="算法" scheme="xiaodouhua.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="ML" scheme="xiaodouhua.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法5_朴素贝叶斯法</title>
    <link href="xiaodouhua.github.io/2017/05/02/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%955_%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/"/>
    <id>xiaodouhua.github.io/2017/05/02/统计学习方法5_朴素贝叶斯法/</id>
    <published>2017-05-02T14:20:18.000Z</published>
    <updated>2017-07-01T12:21:27.196Z</updated>
    
    <content type="html"><![CDATA[<h2 id="朴素贝叶斯法"><a href="#朴素贝叶斯法" class="headerlink" title="朴素贝叶斯法"></a>朴素贝叶斯法</h2><p>朴素贝叶斯 (naive Bayes) 法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布；然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出Y。</p>
<h3 id="朴素贝叶斯法的学习与分类"><a href="#朴素贝叶斯法的学习与分类" class="headerlink" title="朴素贝叶斯法的学习与分类"></a>朴素贝叶斯法的学习与分类</h3><p><strong>基本方法</strong></p>
<p>朴素贝叶斯法通过训练数据集学习X和Y的联合概率分布P(X,Y)。</p>
<p>具体地，学习以下先验概率分布及条件概率分布。</p>
<p><strong>先验概率分布</strong></p>
<p><img src="http://images0.cnblogs.com/blog/790160/201507/281255230317660.png" alt="img"></p>
<p><strong>条件概率分布</strong></p>
<p><img src="http://images0.cnblogs.com/blog/790160/201507/281255233913617.png" alt="img"></p>
<p>条件概率分布有指数级数量的参数，其估计实际是不可行的。朴素贝叶斯法对条件概率分布作了<strong>条件独立性</strong>的假设。条件独立性假设是说用于分类的特征在类确定的条件下都是条件独立的。</p>
<p><img src="http://images0.cnblogs.com/blog/790160/201507/281255237038288.png" alt="img"></p>
<p>朴素贝叶斯法实际上学习到生成数据的机制，所以属于生成模型。</p>
<p>朴素贝叶斯法通过最大后验概率(MAP)准则进行类的判决，基于贝叶斯定理，后验概率为：</p>
<p><img src="http://images0.cnblogs.com/blog/790160/201507/281255245789387.png" alt="img"></p>
<p>分母相同，则分类器可表示为<img src="http://images0.cnblogs.com/blog/790160/201507/281255255942743.png" alt="img"> </p>
<p>后验概率最大化等价于0-1损失函数时的期望风险最小化。</p>
<h3 id="朴素贝叶斯法的参数估计"><a href="#朴素贝叶斯法的参数估计" class="headerlink" title="朴素贝叶斯法的参数估计"></a>朴素贝叶斯法的参数估计</h3><p><strong>极大似然估计</strong></p>
<p> 先验概率的极大似然估计：<img src="http://images0.cnblogs.com/blog/790160/201507/281255259389172.png" alt="img">  </p>
<p>设第j个特征x(j)可能取值的集合为<img src="http://images0.cnblogs.com/blog/790160/201507/281255262816601.png" alt="img">，条件概率的极大似然估计：<img src="http://images0.cnblogs.com/blog/790160/201507/281255265634814.png" alt="img">   </p>
<p>总结算法：</p>
<p><img src="http://images0.cnblogs.com/blog/790160/201507/281255269537229.png" alt="img"></p>
<p>贝叶斯估计</p>
<p>用极大似然估计可能会出现所要估计的概率值为0的情况，使分类产生偏差，解决这一问题的方法是采用贝叶斯估计。条件概率的贝叶斯估计为：</p>
<p><img src="http://images0.cnblogs.com/blog/790160/201507/281255273752401.png" alt="img"></p>
<p>式中$lamda$&gt;=0。等价于在随机变量各个取值的频数上赋予一个正数。常取lamda=1，称为拉普拉斯平滑( Laplace smoothing)。同样，先验概率的贝叶斯估计为：</p>
<p><img src="http://images0.cnblogs.com/blog/790160/201507/281255276104628.png" alt="img"></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;朴素贝叶斯法&quot;&gt;&lt;a href=&quot;#朴素贝叶斯法&quot; class=&quot;headerlink&quot; title=&quot;朴素贝叶斯法&quot;&gt;&lt;/a&gt;朴素贝叶斯法&lt;/h2&gt;&lt;p&gt;朴素贝叶斯 (naive Bayes) 法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集
    
    </summary>
    
      <category term="读书" scheme="xiaodouhua.github.io/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="算法" scheme="xiaodouhua.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="ML" scheme="xiaodouhua.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法4_K近邻法</title>
    <link href="xiaodouhua.github.io/2017/05/01/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%954_K%E8%BF%91%E9%82%BB%E6%B3%95/"/>
    <id>xiaodouhua.github.io/2017/05/01/统计学习方法4_K近邻法/</id>
    <published>2017-05-01T14:20:18.000Z</published>
    <updated>2017-07-01T12:18:18.258Z</updated>
    
    <content type="html"><![CDATA[<h1 id="K近邻法"><a href="#K近邻法" class="headerlink" title="K近邻法"></a>K近邻法</h1><p>k近邻法（k-nearest neighbor, k-NN）是一种基本<strong>分类与回归方法</strong>。这里只讨论分类问题中的k近邻法。k近邻法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。k近邻法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方法进行预测。当K=1时，就是我们所熟悉的最近邻方法（NN）。<strong>因此，k近邻法不具有显式的学习过程</strong>。k近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。<strong>k值的选择、距离度量及分类决策规则是k近邻法的三个基本要素</strong>。</p>
<h3 id="k近邻算法"><a href="#k近邻算法" class="headerlink" title="k近邻算法"></a><strong>k近邻算法</strong></h3><p>输入：训练数据集:$T={ (x_1,y_1),(x_2,y_2),\cdots (x_N,y_N)}$,其中,$x_i\in R^n$为实例的特征向量，$y_i\in{c_1,c_2,\cdots c_k}$ 为实例的类别。</p>
<p>输出：实例x所属的类y</p>
<p>（1）根据给定的距离度量，在训练集T中找出与x最近邻的k个点，涵盖这k个点的邻域记作$N_k(x)$;<br>（2）在$N<em>k(x)$中根据分类决策规则（如多数表决）决定x的类别y：$y=arg max</em>{c<em>j} \sum</em>{x_i\in N_k(x)}I(y_i=c_j),i=1,2,\cdots N,j=1,2,\cdots K$</p>
<p>由这个简单的算法过程可以看出来，距离的选择、以及k的选择都是很重要的，这恰好对应的三个要素中的两个，另一个为分类决策规则，一般来说是多数表决法。</p>
<h3 id="k近邻模型"><a href="#k近邻模型" class="headerlink" title="k近邻模型"></a><strong>k近邻模型</strong></h3><p>k近邻算法使用的模型实际上对应于特征空间的划分，模型由三个基本要素——距离度量、k值的选择和分类决策规则决定。</p>
<p>KNN是一种memory-based learning，也叫instance-based learning，属于lazy learning。即它没有明显的前期训练过程，而是程序开始运行时，把数据集加载到内存后，不需要进行训练，就可以开始分类了。 具体是每次来一个未知的样本点，就在附近找K个最近的点进行投票。</p>
<p><strong>相当于把特征空间划分为许多块。</strong></p>
<h4 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a><strong>距离度量</strong></h4><p>特征空间中俩个实例的距离是俩个实例点相似程度的反映，k近邻中一般使用欧氏距离，本文中主要只介绍这一种。</p>
<p>设特征空间<img src="http://images.cnitblog.com/i/499011/201406/302246246522325.jpg" alt="img">是<img src="http://images.cnitblog.com/i/499011/201406/302247309499245.jpg" alt="img">维实数向量空间<img src="http://images.cnitblog.com/i/499011/201406/302249026847771.jpg" alt="img">，<img src="http://images.cnitblog.com/i/499011/201406/302249518096479.jpg" alt="img">，<img src="http://images.cnitblog.com/i/499011/201406/302251140592141.jpg" alt="img">,<img src="http://images.cnitblog.com/i/499011/201406/302252117773340.jpg" alt="img">,<img src="http://images.cnitblog.com/i/499011/201406/302253023878535.jpg" alt="img">的<img src="http://images.cnitblog.com/i/499011/201406/302255346523020.jpg" alt="img">距离定义为<img src="http://images.cnitblog.com/i/499011/201406/302256308407419.jpg" alt="img"></p>
<p>当p=2时，称为欧氏距离(Euclidean distance).</p>
<h4 id="K值的选择"><a href="#K值的选择" class="headerlink" title="K值的选择"></a><strong>K值的选择</strong></h4><p>K值的选择对最终的结果有很大的影响.</p>
<p>如果选择较小的k值，就相当于用较小的领域中的训练实例进行预测，“学习”的近似误差（approximation error）会减小，只有与输入实例相近的（相似的）训练实例才会对预测结果起作用。但是“学习”的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，<strong>k值的减小就意味着整体模型变得复杂，容易发生过拟合</strong>。<br>如果选择较大的k值，就相当于用较大邻域中的训练实例进行预测。其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的（不相似的）训练实例也会对预测起作用，使预测发生错误。<strong>k值的增大意味着整体的模型变得简单</strong>。<br>如果k=N，那么无论输入实例是什么，都将简单的预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的。<br>在应用中，<strong>k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k值</strong>。</p>
<h4 id="分类决策规则"><a href="#分类决策规则" class="headerlink" title="分类决策规则"></a><strong>分类决策规则</strong></h4><p><strong>k近邻法中的分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。</strong><br><strong>多数表决规则等价于经验风险最小化。</strong></p>
<h3 id="实现-构造kd树"><a href="#实现-构造kd树" class="headerlink" title="实现-构造kd树"></a><strong>实现-构造kd树</strong></h3><p>kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。 </p>
<h4 id="构造平衡kd树"><a href="#构造平衡kd树" class="headerlink" title="构造平衡kd树"></a><strong>构造平衡kd树</strong></h4><p>输入：k维空间数据集$T={ x_1,x_2,\cdots, x_N}$， 其中$x_i=(x_i^{(1)},x_i^{(2)}, \cdots x_i^{(k)})^T,i=1,2,\cdots,N;$;<br>输出：kd树。<br>（1）开始：构造根结点，根结点对应于包含T的k维空间的超矩形区域。<br>选择以$x^{(1)}$为坐标轴，以T中所有的实例的$x^{(1)}$坐标的<strong>中位数</strong>为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。<br>由根结点生成深度为1的左、右子结点：左子结点对应左边$x^{(1)}$小于切分点的子区域，右子结点对应于坐标$x^{(1)}$大于切分点的子区域。<br>将落在切分超平面上的实例点保存在根结点。<br>（2）重复：对深度为j的结点，选择$x^{(l)}$为切分的坐标轴，$l=j(mod )k+1$，以该结点的区域中的所有实例的$x^{(l)}$坐标的中位数为切分点，将该结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(l)}$垂直的超平面实现。<br>由该结点生成的深度为j+1的左、右子结点：左子结点对应坐标$x^{(l)}$小于切分点的子区域，右子结点对应坐标$x^{(l)}$大于切分点的子区域。<br>将落在切分超平面上的实例点保存在该结点。<br>（3）直到两个子区域没有实例存在时停止，从而形成kd树的区域划分。</p>
<h4 id="搜索kd树"><a href="#搜索kd树" class="headerlink" title="搜索kd树"></a><strong>搜索kd树</strong></h4><p>利用kd树可以省去对大部分数据点的搜索，从而减少搜索的即使是算量。这里以最近邻（k=1）为例加以叙述，同样的方法可以应用到k近邻。<br><strong>用kd树的最近邻搜索</strong><br>输入：已构造的kd树，目标点x；<br>输出：x的最近邻<br>（1）在kd树中找到包含目标点x的叶结点：从根结点出发，递归地向下访问kd树。若目标点x当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点。直到子结点为叶节点为止。<br>（2）以此叶节点为“当前最近点”<br>（3）递归地向上回退，在每个结点进行以下操作：<br>（a）如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”<br>（b）当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点对应的区域是否与以目标点为球心，以目标点与“当前最近点”间的距离为半径的超球体相交。<br>如果相交，可能在另一个子结点对应的区域内存在距目标更近的点，移动到另一个子结点，接着，递归地进行最近邻搜索：<br>如果不相交，向上回退<br>（4）当回退到根结点时，搜索结束。最后的“当前最近点”即为x的最近邻点。 </p>
<p><img src="http://my.csdn.net/uploads/201206/03/1338711447_6884.gif" alt="img"></p>
<p>如果实例点是随机分布的，kd树搜索的平均计算复杂度是O(logN),这里N是训练实例数。kd树更适合用于训练实例数远大于空间维数时的k近邻搜索，当空间维数接近训练实例数时，它的效率会迅速下降，几乎接近线性扫描。**</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;K近邻法&quot;&gt;&lt;a href=&quot;#K近邻法&quot; class=&quot;headerlink&quot; title=&quot;K近邻法&quot;&gt;&lt;/a&gt;K近邻法&lt;/h1&gt;&lt;p&gt;k近邻法（k-nearest neighbor, k-NN）是一种基本&lt;strong&gt;分类与回归方法&lt;/strong&gt;。这里
    
    </summary>
    
      <category term="读书" scheme="xiaodouhua.github.io/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="算法" scheme="xiaodouhua.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="ML" scheme="xiaodouhua.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法3_感知机</title>
    <link href="xiaodouhua.github.io/2017/04/26/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%953_%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <id>xiaodouhua.github.io/2017/04/26/统计学习方法3_感知机/</id>
    <published>2017-04-26T14:20:18.000Z</published>
    <updated>2017-07-01T12:25:51.181Z</updated>
    
    <content type="html"><![CDATA[<h1 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h1><p>感知机（perception）是一种<strong>二类分类</strong>的<strong>线性分类</strong>模型<br>输入：实例的特征向量<br>输出：实例的类别（+1，-1）<br>感知机：输入空间中将实例划分为正负两类的<strong>分离超平面</strong>，属于<strong>判别模型</strong><br>感知机学习<br>目的：求出将训练数据进行线性划分的分离超平面<br>方法：<strong>导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型</strong></p>
<ul>
<li><p>2.1感知机模型<br>感知机： </p>
<p>$f(x)=sign(w⋅x+b)$</p>
<p>其中：w为权值（或权值向量weight vector, b为偏值（bias）,sign为符号函数</p>
</li>
</ul>
<p>$ f(n)= \begin{cases}+1, &amp; \text {n&gt;=0} \ -1, &amp; \text{n&lt;0} \end{cases} $</p>
<ul>
<li><p>2.2感知机学习策略<br><strong>数据集线性可分</strong>：存在某个超平面S能够将数据集T的正负实例点完全正确地划分到超平面的两侧，即：<br>对所有yi=+1的实例，有w⋅xi+b&gt;0<br>对所有yi=−1的实例，有w⋅xi+b&lt;0<br>则称数据集T为线性可分数据集。<br><strong>损失函数</strong>的选择：<br>1：误分类点总数，该函数不是参数w，b的连续可导函数，不易优化<br>2：<strong>误分类点到超平面S的总距离</strong> </p>
<p><img src="http://img.my.csdn.net/uploads/201212/23/1356265741_5071.jpg" alt="img"></p>
<p>损失函数是非负的，如果没有误分类点，损失函数值为0。</p>
<p>误分类点越少，误分类点离超平面越近，损失函数值就越小，损失函数是w，b的连续可导函数。</p>
</li>
</ul>
<ul>
<li><p>2.3感知机学习算法<br>原始形式、对偶形式 </p>
<p>随机梯度下降法：<br>任意选取一个超平面w0,b0,利用梯度下降法不断极小化目标函数（极小化的过程是一次随机选取一个误分类点使其梯度下降）<br><strong>随机选取误分类点(xi,yi)</strong>,对w，b进行更新： （随机梯度下降）  </p>
<p>原始形式：</p>
<p>每来一个样本 (xi,yi)，计算若yi(w⋅xi+b)≤0，</p>
<p>说明误分了，要用梯度下降：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">w=w+ηyixi</span><br><span class="line"></span><br><span class="line">b=b+ηyi</span><br><span class="line"></span><br><span class="line">其中，η(0&lt;η≤1)为步长（学习率）</span><br></pre></td></tr></table></figure>
<p>更新权重 w,b</p>
<p>若大于0，说明分类正确，不用管。这样直到遍历整个样本都没有误分点，算法停止。  通过迭代可使损失函数    L(w,b)  不断减小，直至0</p>
<p>​</p>
<p>通过观察上面梯度下降的更新公式，发现参数 w 最终的取值，其实是初值（初值为0可以忽略）加上每个样本的某个整数倍（其实就是分错的次数）</p>
<p>$$\begin{cases} w = \sum_{i=1}^{N}\alpha_iy_ix<em>i  \b  = \sum</em>{i=1}^{N}\alpha_iy_i  \end{cases}$$</p>
<p>其中，$\alpha_i$≥0, i=1,2,…,N 表示每个样本点在更新过程中被误分类的次数。从 α 中我们还可以看出来，若该值越大，则说明对应的样本点被误分的次数越大，也就离超平面越近，因此难分类。</p>
<p>判断是否属于误分点时，我们用公式$y<em>i(\sum</em>{j=1}^n\alpha_jy_jx_j⋅x_i+b)≤0​$来判断。对比之前的 w,b 版本，其实只是把 w 用 α代替而已。注意后面的点乘，即内积，我们可以先事先计算好并存储在Gram矩阵中$（G=[x_i⋅x<em>j]</em>{N×N}）​$，以减少实时计算量。</p>
<p>​</p>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;感知机&quot;&gt;&lt;a href=&quot;#感知机&quot; class=&quot;headerlink&quot; title=&quot;感知机&quot;&gt;&lt;/a&gt;感知机&lt;/h1&gt;&lt;p&gt;感知机（perception）是一种&lt;strong&gt;二类分类&lt;/strong&gt;的&lt;strong&gt;线性分类&lt;/strong&gt;模型&lt;br
    
    </summary>
    
      <category term="读书" scheme="xiaodouhua.github.io/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="算法" scheme="xiaodouhua.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="ML" scheme="xiaodouhua.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法2</title>
    <link href="xiaodouhua.github.io/2017/04/23/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%952/"/>
    <id>xiaodouhua.github.io/2017/04/23/统计学习方法2/</id>
    <published>2017-04-23T14:20:18.000Z</published>
    <updated>2017-07-01T13:53:37.081Z</updated>
    
    <content type="html"><![CDATA[<p>监督学习的目的是找到一个输入输出映射（模型），使得这个模型不仅对训练数据有很好的拟合能力，对于未知数据，它也有很好的预测能力。这篇博客介绍选择和评估模型的标准。本篇博客一共有以下几个重点：训练误差与测试误差、泛化能力、过拟合。</p>
<h2 id="训练误差与测试误差"><a href="#训练误差与测试误差" class="headerlink" title="训练误差与测试误差"></a>训练误差与测试误差</h2><p><strong>训练误差</strong>是模型$Y={\hat{f}}(X))$关于训练集的平均损失来表示：$R<em>{emp}({\hat{f}})=\frac{1}{N}\sum</em>{i=1}^{N}L(y<em>{i},{\hat{f}}(x</em>{i}))$</p>
<p>其中$N$为训练集中数据的个数；损失函数$L(Y,f(X))$在统计学三要素中有介绍，可以看到，训练误差也就是该篇文章所提到的经验损失函数。训练误差的大小，可以说明问题是不是一个容易学习的问题。</p>
<p><strong>测试误差</strong>是模型$Y={\hat{f}}(X)$关于测试集数据的平均损失：<br>$$R<em>{test}({\hat{f}})=\frac{1}\sum</em>{i=1}^{N’}L(y<em>i, {\hat{f}}(x</em>{i}))$$</p>
<p>其中${N}’$为测试集中数据的个数。测试误差反应了学习模型对于未知的测试集中数据的预测能力。由于测试误差只是表征了对于测试集中数据的预测能力，我们可以更一般的引申一下，由此引入了模型对于所有未知数据预测能力——泛化能力。</p>
<p>$$e<em>{test} = \frac{1}{N’}\sum</em>{i=1}^N’ I(y_i \neq f(x_i))$$</p>
<p>其中 $I(\cdot)$ 为指标函数（indicator function），当$\cdot$为真是返回$1$ ，否则返回 $0$；$N’$ 为测试样本容量，测试准确率（或称为精度（accuracy））：$acc<em>{test} = 1 - e</em>{test}$</p>
<h2 id="泛化能力"><a href="#泛化能力" class="headerlink" title="泛化能力"></a>泛化能力</h2><h3 id="泛化误差"><a href="#泛化误差" class="headerlink" title="泛化误差"></a>泛化误差</h3><p>泛化能力用来表征学习模型对于未知数据的预测能力。</p>
<p>很显然，我们没有办法对所有的数据进行预测然后判断来计算一个模型的模型的泛华能力，所以在<strong>实际应用</strong>当中，我们一般还是用的测试集中的数据来近似泛化能力。</p>
<p>但是，统计学习也在<strong>理论上</strong>试图对学习方法的泛化能力进行了分析，首先给出泛化误差的定义：用学到的$\hat{f}$对未知数据进行预测：  </p>
<p>$R<em>{exp}(\hat{f})=E[L(Y,\hat{f}(X))]=\int L(y</em>{i},\hat{f}(x_{i}))P(x,y)dxdy$</p>
<p>泛化误差越小，学习模型的泛化能力越好。可以看到，泛化误差实际上就是统计学三要素中介绍的风险函数。由上篇博客我们可知，泛化误差（风险函数）不可计算。</p>
<h3 id="过拟合与欠拟合"><a href="#过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合</h3><p>训练误差很小而泛化误差很大时称为过拟合，与之相对的是<strong>欠拟合（underfitting）</strong>。例如多项式拟合：</p>
<p>$$f_M(x, \omega) = \omega_0 + \omega_1x + \omega_2x^2+\dots+\omega<em>Mx^M = \sum</em>{j=0}^M\omega_jx^j$$</p>
<p>当选取 $M$ 个参数进行训练时，可能出现下列情况：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/21342-d4f50bd92c98bf51.png?imageMogr2/auto-orient/strip%7CimageView2/2" alt="fitting"></p>
<p>当 $M = 0$ 和 $M = 1$ 时，模型为直线，拟合效果很差，即欠拟合；当 $M = 9$ 时，模型曲线经过了每一个训练数据点，训练误差为 0，但是无法预测新的数据，因此泛化误差很大，即过拟合。</p>
<h3 id="测试误差的评估方法"><a href="#测试误差的评估方法" class="headerlink" title="测试误差的评估方法"></a>测试误差的评估方法</h3><ol>
<li>留出法（hold-out）</li>
<li>交叉验证法（cross validation）</li>
<li>自助法（bootstrapping）</li>
<li>调参（parameter tuning）</li>
</ol>
<h4 id="留出法"><a href="#留出法" class="headerlink" title="留出法"></a>留出法</h4><p>将数据集 $D$ 划分为 $S, T$：</p>
<p>$$D = S \cap T, S \cup T = \emptyset$$</p>
<p>并采用<strong>分层采样（stratified sampling）</strong>，通常选用 $2/3 - 4/5$ 用于训练</p>
<h4 id="交叉验证法"><a href="#交叉验证法" class="headerlink" title="交叉验证法"></a>交叉验证法</h4><p>将 $D$ 划分为 $k$ 个大小相似的互斥子集：</p>
<p>$$D = D_1 \cup D_2 \cup \dots \cup D_k, D_i \cap D_j = \emptyset (i \neq j)$$</p>
<p>每次用 $k-1$ 个子集作为训练集，剩下一个作为测试集，称为<strong>k折交叉验证（k-fold cross validation）</strong>。$k$ 通常取 10，并随机使用不同划分重复 $p$ 次，最终取 $p$ 次结果均值，例如“10次10折交叉验证”。</p>
<p>假设数据集 $D$ 容量为 $m$，若 $k = m$，则称为<strong>留一法（Leave-One-Out, LOO）</strong>。留一法苹果结果比较准确，但计算开销也相应较大。</p>
<h4 id="自助法"><a href="#自助法" class="headerlink" title="自助法"></a>自助法</h4><p>以<strong>自助采样法（bootstrap sampling）</strong>为基础，从 $D$ 中有放回地随机抽取 $m$ 次，得到同样包含 $m$ 个样本的 $D’$，$D$ 中有一部分样本会在 $D’$ 中出现多次，而另一部分则未出现，$m$ 次重采样始终未被采到的概率是：</p>
<p>$$\lim_{m\rightarrow\infty}(1-\frac{1}{m})^m \rightarrow \frac{1}{e} \approx 0.368$$,即 $36.8\%$ 的样本未出现在 $D’$。以 $D’$ 作为训练集，$D - D’$ 作为测试集。自助法在数据集较小、难以划分训练/测试集时很有用。</p>
<h3 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h3><p>除了精度（$acc<em>{test}$）和错误率（$e</em>{test}$），还需要反映任务需求的性能度量指标。</p>
<ol>
<li>查准率、查全率与 $F_1$</li>
<li>ROC &amp; AUC</li>
<li>代价矩阵</li>
</ol>
<h4 id="查准率、查全率与-F-1"><a href="#查准率、查全率与-F-1" class="headerlink" title="查准率、查全率与 $F_1$"></a>查准率、查全率与 $F_1$</h4><p><img src="http://upload-images.jianshu.io/upload_images/21342-cd4eb53323e1ac52.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="img"></p>
<p>$$ TP + FP + TN + FN = 1$$</p>
<p>查准率（准确率，precision）：<br>$$P = \frac{TP}{TP + FP}$$</p>
<p>查全率（召回率，recall）：<br>$$R = \frac{TP}{TP + FN}$$</p>
<p>希望查全率高，意味着更看重决策的准确性，例如在商品推荐系统，尽量减少错误推荐；希望查全率高，意味着“宁可错杀一千”，例如在罪犯检测过程中。</p>
<p>$$F<em>1 = \frac{2PR}{P+R}$$<br>$$F</em>\beta = \frac{(1+\beta^2)PR}{(\beta^2+P)+R}$$</p>
<p>当 $\beta = 1$ 时，$F_\beta = F_1$；$\beta \gt 1$ 时，查全率影响更大；$\beta \lt 1$ </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;监督学习的目的是找到一个输入输出映射（模型），使得这个模型不仅对训练数据有很好的拟合能力，对于未知数据，它也有很好的预测能力。这篇博客介绍选择和评估模型的标准。本篇博客一共有以下几个重点：训练误差与测试误差、泛化能力、过拟合。&lt;/p&gt;
&lt;h2 id=&quot;训练误差与测试误差&quot;&gt;
    
    </summary>
    
      <category term="读书" scheme="xiaodouhua.github.io/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="算法" scheme="xiaodouhua.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="ML" scheme="xiaodouhua.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法1</title>
    <link href="xiaodouhua.github.io/2017/04/22/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%951/"/>
    <id>xiaodouhua.github.io/2017/04/22/统计学习方法1/</id>
    <published>2017-04-22T12:20:18.000Z</published>
    <updated>2017-07-01T12:25:48.165Z</updated>
    
    <content type="html"><![CDATA[<h2 id="统计学习方法-模型-策略-方法"><a href="#统计学习方法-模型-策略-方法" class="headerlink" title="统计学习方法=模型+策略+方法"></a><strong>统计学习方法=模型+策略+方法</strong></h2><p>统计学习方法之间的不同，主要来自于其模型、策略、算法的不同。确定了模型、策略和算法，统计学习方法也就确定了。</p>
<p><strong>Note:</strong> 以下以监督学习为基础来进行论述。非监督学习和强化学习同样也拥有这三要素。  </p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>在监督学习当中，我们的<strong>目的是学习一个由输入到输出的映射，这个映射就是模型</strong>。一般来说，模型有两种形式，一种是概率模型（由条件概率分布表示的模型），另一种形式是非概率模型（由决策函数表示的模型）。我们根据实际情况和具体的学习方法来决定是用概率模型还是用非概率模型。   </p>
<p>模型的假设空间（hypothesis space）是一集合：由输入空间到输出空间所有映射的集合，包含所有可能的条件概率分布或决策函数。  </p>
<p>假设空间${ F}={ f|Y=f(X)} $,这时候${\scr F}$是决策函数的集合，由参数向量决定： ${F}={ f|Y=f_\theta(X)}$，$\theta$取值于n维欧式空间$R^n$,称为参数空间  。</p>
<p>表示为条件概率集合${ F}={ P|P(Y|X)} $,${F}$是一个参数向量决定的条件概率分布簇。  </p>
<h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><p>统计学习的目标在于从假设空间中选取最优模型。策略即为按照什么样的准则或方法来找到这个最优模型。首先介绍损失函数和风险函数。 </p>
<ul>
<li><strong>损失函数（代价函数）： 一次预测的好坏</strong></li>
<li><strong>风险函数： 平均意义下模型预测的好坏</strong> </li>
</ul>
<p>包括：</p>
<p>损失函数：</p>
<ul>
<li>0-1损失函数 0-1 loss function</li>
<li>平方损失函数 quadratic loss function</li>
<li>绝对损失函数 absolute loss function</li>
<li>对数损失函数 </li>
</ul>
<p><img src="http://img.blog.csdn.net/20160725111234270" alt="这里写图片描述"><br><strong>2）风险函数</strong></p>
<p><img src="http://img.blog.csdn.net/20160725111410273" alt="这里写图片描述"></p>
<p>模型f(x)关于训练数据集的平均损失称为经验风险（empirical risk）或经验损失（empirical loss），记作$R_{emp} $.</p>
<p><img src="http://img.blog.csdn.net/20160725111432874" alt="这里写图片描述"> </p>
<p>期望风险Rexp(f)是模型关于联合分布的期望损失，经验风险Remp(f)是模型关于训练样本集的平均损失。根据大数定律，当样本容量N趋于无穷时，经验风险Rempf(x)趋于期望风险Rexpf(x)，所以一个很自然的想法是用经验风险估计期望风险。但是，<strong>由于现实中训练样本数目有限甚至很小，所以用经验风险估计期望风险常常并不理想，常常会导致过拟合。为了防止过拟合现象，结构风险最小化这个策略被提了出来。</strong></p>
<p>当样本容量很小时， 经验风险最小化学习的效果未必很好， 会产生“ 过拟合over-fitting”。</p>
<p>结构风险最小化 structure risk minimization， 为防止过拟合提出的策略， 等价于正则化（ regularization） ， 加入正则化项regularizer， 或罚项 penalty term。惩罚项的大小与模型复杂度有关。</p>
<p><strong>3)经验风险最小化</strong></p>
<p>在假设空间，损失函数以及训练数据集确定的情况下，经验风险函数式就可以确定，经验风险最小化（empirical risk<br>minimizatiion, ERM）的策略认为，经验风险最小的模型是最优模型。 <img src="http://img.blog.csdn.net/20160725111551109" alt="这里写图片描述">   </p>
<p>当样本容量是够大时，经验风险最小化能保证有很好的学习效果，在现实中被广泛应用，比如，极大似然估计（maximum likelihood estimation）就是经验风险最小化的一个例子，当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。但是，当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生“过拟合（over-fitting）”现象。</p>
<p><strong>4)结构化风险最小化</strong></p>
<p>结构化风险最小化（structural risk minimization, SRM）是为了防止过拟合而提出来的策略。结构风险在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term）。在假设空间，损失函数以及训练数据集确定的情况下，结构风险的定义是：<br><img src="http://img.blog.csdn.net/20160725111735667" alt="这里写图片描述"><br>其中J(f)为模型的复杂度，是定义在假设空间 F 上的泛函，模型 f 越复杂，复杂度J(f)就越大；反之，模型 f 越简单，复杂度J(f)就越小，也就是说，<strong>复杂度表示了对复杂模型的惩罚</strong>，λ≥0是系数，用以权衡经验风险和模型复杂度，结构风险小需要经验风险与模型复杂度同时小，结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。<br>结构风险最小化的策略认为结构风险最小的模型是最优的模型：<br><img src="http://img.blog.csdn.net/20160725111755532" alt="这里写图片描述"></p>
<p>综上，我们在统计学习中的策略一般有两种——经验风险最小化，结构风险最小化。此时，我们就把统计学习问题转为了求解下面目标函数的优化问题：<br><img src="http://img.blog.csdn.net/20160725111922419" alt="这里写图片描述"> 或者： <img src="http://img.blog.csdn.net/20160725111940248" alt="这里写图片描述"></p>
<p>综上，我们在统计学习中的策略一般有两种——经验风险最小化，结构风险最小化。此时，我们就把统计学习问题转为了求解下面目标函数的优化问题：<br><img src="http://img.blog.csdn.net/20160725111922419" alt="这里写图片描述"> 或者： <img src="http://img.blog.csdn.net/20160725111940248" alt="这里写图片描述"></p>
<h2 id="模型评估与模型选择"><a href="#模型评估与模型选择" class="headerlink" title="模型评估与模型选择"></a>模型评估与模型选择</h2><p>算法是指学习模型的具体计算方法，统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方式求解最优模型。这时，统计学习问题归结为最优化问题，统计学习的方法成为求解最优化问题的算法。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;统计学习方法-模型-策略-方法&quot;&gt;&lt;a href=&quot;#统计学习方法-模型-策略-方法&quot; class=&quot;headerlink&quot; title=&quot;统计学习方法=模型+策略+方法&quot;&gt;&lt;/a&gt;&lt;strong&gt;统计学习方法=模型+策略+方法&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;统
    
    </summary>
    
      <category term="读书" scheme="xiaodouhua.github.io/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="算法" scheme="xiaodouhua.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="ML" scheme="xiaodouhua.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>几道面试笔试题</title>
    <link href="xiaodouhua.github.io/2017/04/06/%E5%87%A0%E9%81%93%E9%9D%A2%E8%AF%95%E7%AC%94%E8%AF%95%E9%A2%98/"/>
    <id>xiaodouhua.github.io/2017/04/06/几道面试笔试题/</id>
    <published>2017-04-06T15:19:55.000Z</published>
    <updated>2017-04-06T15:48:17.568Z</updated>
    
    <content type="html"><![CDATA[<h2 id="老鼠喝药问题"><a href="#老鼠喝药问题" class="headerlink" title="老鼠喝药问题"></a>老鼠喝药问题</h2><p>问题：  </p>
<p>有 1000 个一模一样的瓶子，其中有 999 瓶是普通的水，有一瓶是毒药。任何喝下毒药的生物都会在一星期之后死亡。现在，你只有 10 只小白鼠和一星期的时间，如何检验出哪个瓶子里有毒药？<br><a id="more"></a><br>分析：</p>
<p>直接贴知乎回答吧  </p>
<blockquote>
<p><strong>Q1：为什么10个白鼠能分辨1000瓶液体？</strong><br>（先假设老鼠喝毒药之后立刻死，而不是一周后，稍后解释原因）  </p>
<p>我们让第一只老鼠喝掉前500瓶液体，如果它死了，那么毒药就在前500瓶里。  </p>
<p>下面，将存在毒药的500瓶一分为二，再拿一只老鼠来试验：我们再次精确把毒药限制在250瓶中。  </p>
<p>继续下去，每次取一半做实验，我们将依次精确到125瓶（第三只），精确到62/(或)63瓶（第四只），31/32瓶（第五只），15/16瓶（第六只），7/8瓶（第七只），3/4瓶（第八只），1/2（第九只，这次可能一举就确认，如果恰好出现在只有1瓶的那个分块中，饶过一只可怜的小白鼠，否则就需要再实验一次决定），1瓶（第十只）。    </p>
<p><strong>Q2：上面这个假设没有考虑到“只能观察一次”这个前提，即不允许我们每次实验一下，观察并排除一半，再循环实验。用什么方式让这种分辨在一次观察中就解决呢？</strong><br>（考虑 3 只老鼠，8(2^3) 瓶液体的情况）我们让第一只老鼠喝掉1，2，3，4前四瓶；第二只喝掉1，2，5，6瓶；第三只老鼠喝掉2，4，6，8瓶。<br>一周后，假设发生：第一只老鼠死了：毒药在前4瓶（1,2,3,4）；第二只老鼠活着：毒药在（3,4）；第三只老鼠死了：第4瓶。  </p>
<p><strong>Q3：推广到更多毒药要老鼠怎么喝呢？</strong><br>如题设，1000瓶的情况，我们让每只老鼠都喝掉一半，即500瓶。第一只喝掉前500瓶；第二只每隔250瓶喝接下来的250瓶，即1~250, 501~750；第三只每隔125瓶喝125瓶，即1~125, 251~375, 501~625, 751~875；第四只每隔62瓶喝63瓶（或每隔63瓶喝62瓶）；直到第十只，每隔1瓶喝一瓶，即都喝奇数（或偶数）瓶。<br>比如只有第1, 2, 3, 4, 5, 6, 7, 8, 9只死亡的情况（除了第10只其他都死了），我们能依次精确到毒药存在于以下范围的瓶子中，1-500, 1-250, 1-125, 1-63, 1-32, 1-16, 1-8, 1-4, 1-2, 2。得出第2瓶是毒药的结果。  </p>
<p><strong>Q4：这和二进制的解法有什么区别？</strong><br>没区别。二进制解法中，第一只老鼠，即负责个位的那个老鼠，就是每隔1瓶就喝一个：因为二进制逢 2 归 0；第 3 只老鼠，即负责百位的老鼠，每隔 2^(3-1) = 4 瓶喝 4 个：二进制百位隔 4 个保持一样；第 n 只老鼠，即负责从各位数第 n 位的老鼠，每隔 2^(n-1) 瓶喝  2^(n-1)  个。其实和上述用逻辑的方式喝法没有区别。  </p>
<p>作者：dark blue链接：<a href="https://www.zhihu.com/question/19676641/answer/127149488" target="_blank" rel="external">https://www.zhihu.com/question/19676641/answer/127149488</a>  </p>
<p>来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。  </p>
</blockquote>
<p>解释一下二进制的方法：将瓶子编号为0~1023（10个老鼠可以分辨1024瓶药水），转换为2进制，老鼠标号为1~10号，对每一瓶药水，如果所在的位为1，就要喝。  </p>
<p>比如：00 0000 0001 第一个老鼠喝，所以第一个老鼠是负责第一位的，即每隔一瓶喝一瓶。  </p>
<p>00 0000 0010 和00 0000 0011第二个老鼠要喝，即每隔两瓶喝两个。  </p>
<p>……   </p>
<p>第10只老鼠喝的是10 0000 0000~11 1111 1111的药水，即后一半。  </p>
<p>可以看到，与Q3是一样的，不过顺序相反了。</p>
<hr>
<h2 id="出栈顺序问题"><a href="#出栈顺序问题" class="headerlink" title="出栈顺序问题"></a>出栈顺序问题</h2><p>首先想到的就是著名的  <a href="https://zh.wikipedia.org/wiki/%E5%8D%A1%E5%A1%94%E5%85%B0%E6%95%B0" target="_blank" rel="external">卡特兰数</a>  :  </p>
<p>$C_n=\frac{1}{n+1}\cdot \begin{pmatrix} 2n \ n \ \end{pmatrix}=\frac{(2n)!}{(n+1)!\cdot n!}$  </p>
<p>它的应用很广：  </p>
<ul>
<li><em>$C_n$表示长度</em>2n<em>的dyck word的个数。Dyck word是一个有</em>n<em>个X和</em>n*个Y组成的字串，且所有的前缀字串皆满足X的个数大于等于Y的个数。以下为长度为6的dyck words:</li>
</ul>
<p>XXXYYY XYXXYY XYXYXY XXYYXY XXYXYY</p>
<ul>
<li>将上例的X换成左括号，Y换成右括号，<em>$C_n$</em>表示所有包含<em>n</em>组括号的合法运算式的个数：</li>
</ul>
<p>((())) ()(()) ()()() (())() (()())</p>
<ul>
<li><em>$C_n$</em>表示有<em>n</em>个节点组成不同构<a href="https://zh.wikipedia.org/wiki/%E4%BA%8C%E5%8F%89%E6%A0%91" target="_blank" rel="external">二叉树</a>的方案数。下图中，<em>n</em>等于<em>3</em>，圆形表示节点，月牙形表示什么都没有。</li>
</ul>
<ul>
<li><em>$C_n$</em>表示有<em>2n+1</em>个节点组成不同构满<a href="https://zh.wikipedia.org/wiki/%E4%BA%8C%E5%8F%89%E6%A0%91" target="_blank" rel="external">二叉树</a>（full binary tree）的方案数。下图中，<em>n</em>等于<em>3</em>，圆形表示内部节点，月牙形表示外部节点。本质同上。<img src="https://zh.wikipedia.org/wiki/File:Catalan_number_binary_tree_example.png" alt=""></li>
<li><em>$C_n$</em>表示所有在<em>n</em> × <em>n</em>格点中不越过对角线的<strong>单调路径</strong>的个数。一个单调路径从格点左下角出发，在格点右上角结束，每一步均为向上或向右。计算这种路径的个数等价于计算Dyck word的个数：X代表“向右”，Y代表“向上”。下图为<em>n</em> = 4的情况：<img src="https://zh.wikipedia.org/wiki/File:Catalan_number_4x4_grid_example.svg" alt=""></li>
<li><em>$C_n$</em>表示对{1, …, <em>n</em>}依序进出<a href="https://zh.wikipedia.org/wiki/%E6%A0%88" target="_blank" rel="external">栈</a>的<a href="https://zh.wikipedia.org/wiki/%E7%BD%AE%E6%8D%A2" target="_blank" rel="external">置换</a>个数。一个置换<em>w</em>是依序进出栈的当<em>S</em>(<em>w</em>) = (1, …, <em>n</em>),其中<em>S</em>（<em>w</em>）递归定义如下：令<em>w</em> = <em>unv</em>，其中<em>n</em>为<em>w</em>的最大元素，<em>u</em>和<em>v</em>为更短的数列；再令<em>S</em>(<em>w</em>) = <em>S</em>(<em>u</em>)<em>S</em>(<em>v</em>)<em>n</em>，其中<em>S</em>为所有含一个元素的数列的单位元。  </li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;老鼠喝药问题&quot;&gt;&lt;a href=&quot;#老鼠喝药问题&quot; class=&quot;headerlink&quot; title=&quot;老鼠喝药问题&quot;&gt;&lt;/a&gt;老鼠喝药问题&lt;/h2&gt;&lt;p&gt;问题：  &lt;/p&gt;
&lt;p&gt;有 1000 个一模一样的瓶子，其中有 999 瓶是普通的水，有一瓶是毒药。任何喝下毒药的生物都会在一星期之后死亡。现在，你只有 10 只小白鼠和一星期的时间，如何检验出哪个瓶子里有毒药？&lt;br&gt;
    
    </summary>
    
      <category term="技术" scheme="xiaodouhua.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="算法" scheme="xiaodouhua.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>python学习笔记</title>
    <link href="xiaodouhua.github.io/2016/11/21/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>xiaodouhua.github.io/2016/11/21/python学习笔记/</id>
    <published>2016-11-21T07:57:23.000Z</published>
    <updated>2016-11-23T03:25:39.069Z</updated>
    
    <content type="html"><![CDATA[<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p>应用场景:<br>首选是<strong>网络</strong>应用，包括网站、后台服务等等；其次是许多日常需要的<strong>小工具</strong>，包括系统管理员需要的脚本任务等等；另外就是把其他语言开发的<strong>程序再包装</strong>起来，方便使用。  </p>
<p>优缺点：<br>优点：“优雅”、“明确”、“简单”、“内置电池（ batteries included）”（各种库），所以开发速度快。<br>缺点：运行慢，源码公开。<br><a id="more"></a>  </p>
<h2 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h2><p><code>!/usr/bin/env python3 #为了在linux上运行</code><br><code># -*- coding: utf-8 -*- #编码声明</code>  </p>
<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><ul>
<li>整数 <code>1</code>  <code>0xff00</code> etc.</li>
<li>浮点数  <code>3.14</code> <code>3e-5</code> </li>
<li>字符串 <code>&quot;abc&quot;</code> <code>&#39;abc&#39;</code>  <code>r&#39;a\b\vc\\cc\\&#39; # 非转义</code>  、</li>
<li>布尔值 <code>True</code> <code>False</code>,可进行<code>and or not</code>运算  </li>
<li>空值 <code>None</code>  </li>
<li>常量 <code>PI=3.1415926 #默认大写，其实也是变量</code>  </li>
<li>在计算机内存中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，就转为UTF-8编码.在最新的Python 3版本中，字符串是以Unicode编码的    </li>
<li><p>Python提供了<code>ord()</code>函数获取字符的整数表示，<code>chr()</code>函数把编码转换为对应的字符： </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;ord(<span class="string">'A'</span>)  </span><br><span class="line"><span class="number">65</span>  </span><br><span class="line">&gt;&gt;&gt;chr(<span class="number">65</span>)</span><br><span class="line"><span class="string">'A'</span></span><br><span class="line">&gt;&gt;&gt;chr(<span class="number">25991</span>)</span><br><span class="line"><span class="string">'文'</span> </span><br><span class="line">&gt;&gt;&gt;<span class="string">'\u4e2d\u6587'</span> <span class="comment"># 16进制的str  </span></span><br><span class="line"><span class="string">'中文'</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>以Unicode表示的str通过encode()方法可以编码为指定的bytes.如果我们从网络或磁盘上读取了字节流，那么读到的数据就是bytes。要把bytes变为str，就需要用decode()方法 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'ABC'</span>.encode(<span class="string">'ascii'</span>)</span><br><span class="line"><span class="string">b'ABC'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'中文'</span>.encode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">b'\xe4\xb8\xad\xe6\x96\x87'</span>  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'ABC'</span>.decode(<span class="string">'ascii'</span>)</span><br><span class="line"><span class="string">'ABC'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'\xe4\xb8\xad\xe6\x96\x87'</span>.decode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">'中文'</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>格式化输出   </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'%2d-%02d'</span> % (<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="string">' 3-01'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'%.2f'</span> % <span class="number">3.1415926</span></span><br><span class="line"><span class="string">'3.14'</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>列表 <code>classmates = [&#39;Michael&#39;, &#39;Bob&#39;, &#39;Tracy&#39;]</code>.可以下标访问。方法主要有<code>.append(&#39;Lucy&#39;)</code>;<code>.pop()</code>;<code>.pop(i)</code>；<code>classmates.insert(1, &#39;Jack&#39;)</code>里面数据类型可以不同  </p>
</li>
<li>tuple 不可更改的列表<code>classmates = (&#39;Michael&#39;, &#39;Bob&#39;, &#39;Tracy&#39;)</code>  </li>
<li>dic <code>d = {&#39;Michael&#39;: 95, &#39;Bob&#39;: 75, &#39;Tracy&#39;: 85}</code>;方法： <code>d.get(&#39;Thomas&#39;, -1)</code> ;<code>d.pop(&#39;Bob&#39;)</code>  </li>
<li>set <code>s = set([1, 1, 2, 2, 3, 3])</code>;<code>s.add(4)</code>;<code>s.remove(4)</code>;两个set可以做数学意义上的交集、并集等操作  </li>
</ul>
<h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><p>函数主要注意默认参数，可变参数，关键字参数，命名关键字参数。<br>*args是可变参数，args接收的是一个tuple；**kw是关键字参数，kw接收的是一个dict。以及调用函数时如何传入可变参数和关键字参数的语法：可变参数既可以直接传入：<code>func(1, 2, 3)</code>，又可以先组装list或tuple，再通过<code>*args</code>传入：<code>func(*(1, 2, 3))</code>；关键字参数既可以直接传入：<code>func(a=1, b=2)</code>，又可以先组装dict，再通过<code>**kw</code>传入：<code>func(**{&#39;a&#39;: 1, &#39;b&#39;: 2})</code>。使用<code>*args</code>和<code>**kw</code>是Python的习惯写法，当然也可以用其他参数名，但最好使用习惯用法。命名的关键字参数是为了限制调用者可以传入的参数名，同时可以提供默认值。定义命名的关键字参数在没有可变参数的情况下不要忘了写分隔符<code>*</code>，否则定义的将是位置参数。  </p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;综述&quot;&gt;&lt;a href=&quot;#综述&quot; class=&quot;headerlink&quot; title=&quot;综述&quot;&gt;&lt;/a&gt;综述&lt;/h2&gt;&lt;p&gt;应用场景:&lt;br&gt;首选是&lt;strong&gt;网络&lt;/strong&gt;应用，包括网站、后台服务等等；其次是许多日常需要的&lt;strong&gt;小工具&lt;/strong&gt;，包括系统管理员需要的脚本任务等等；另外就是把其他语言开发的&lt;strong&gt;程序再包装&lt;/strong&gt;起来，方便使用。  &lt;/p&gt;
&lt;p&gt;优缺点：&lt;br&gt;优点：“优雅”、“明确”、“简单”、“内置电池（ batteries included）”（各种库），所以开发速度快。&lt;br&gt;缺点：运行慢，源码公开。&lt;br&gt;
    
    </summary>
    
      <category term="技术" scheme="xiaodouhua.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="python" scheme="xiaodouhua.github.io/tags/python/"/>
    
      <category term="编程" scheme="xiaodouhua.github.io/tags/%E7%BC%96%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>git与github学习</title>
    <link href="xiaodouhua.github.io/2016/10/11/git%E4%B8%8Egithub%E5%AD%A6%E4%B9%A0/"/>
    <id>xiaodouhua.github.io/2016/10/11/git与github学习/</id>
    <published>2016-10-10T17:30:23.000Z</published>
    <updated>2016-10-24T17:07:40.705Z</updated>
    
    <content type="html"><![CDATA[<p>别说话，就是干。 </p>
<ul>
<li>[x] 安装</li>
<li>[x] 命令的学习</li>
<li>[x] 与github关联push</li>
<li>[x] git原理的学习<a id="more"></a>
</li>
</ul>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="http://music.163.com/outchain/player?type=2&id=30394763&auto=0&height=66"></iframe>  

<h3 id="安装："><a href="#安装：" class="headerlink" title="安装："></a>安装：</h3><ul>
<li>Mac：<a href="https://sourceforge.net/projects/git-osx-installer/" target="_blank" rel="external">https://sourceforge.net/projects/git-osx-installer/</a></li>
<li>Windows：<a href="https://git-for-windows.github.io/" target="_blank" rel="external">https://git-for-windows.github.io/</a></li>
<li>Linux：<code>apt-get install git</code><br>在任一地方右键<code>git bash</code>即可开始操作。<br>环境变量好像默认是配置成功的，因此在cmd命令行也是可以敲git命令的。<br><img src="/images/cmd_git.png" alt=""></li>
</ul>
<h3 id="命令："><a href="#命令：" class="headerlink" title="命令："></a>命令：</h3><ul>
<li>git config –global user.name “name”</li>
<li>git config –global user.email “XX@example.com”  </li>
<li>git config -l  # 显示配置详情</li>
</ul>
<p>这里设置的姓名和邮件会显示在提交给仓库里的 log 等信息里，与认证等无关。<code>--global</code>代表着表示你这台机器上所有的Git仓库都会使用这个配置，当然也可以对某个仓库指定不同的用户名和Email地址。</p>
<ul>
<li>创建仓库和添加文件到仓库<ul>
<li>mk learngit # 创建文件夹 </li>
<li>cd learngit # 进入文件夹</li>
<li><strong>git init</strong>  # 初始化，将本文件夹作为一个本地仓库 </li>
<li>git add a.txt # 将文件放入index（暂存）区</li>
<li>git commit -m “wrote a readme file” # 提交  </li>
<li>git commit -a -m ‘added new benchmarks’  # 合并了add和commit两条命令  </li>
<li>git commit –amend # 附加提交，与上一次合成一次提交<br>上面这些命令就可以创建文件，提交文件了。注意运行了<code>git add</code>之后又作了修订的文件，需要重新运行 git add 把最新版本重新暂存起来。  </li>
</ul>
</li>
<li>远程仓库的使用：<ul>
<li>git remote add origin git@github.com:yourname/your resposity.git #<code>git remote add [shortname] [url]</code>与一个github库关联，生成一个短名字，以便引用。</li>
<li>git push -u oragin master #<code>git push</code>把本地库的内容推送到远程，用git push命令，实际上是把当前分支master推送到远程。 由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。 </li>
<li>git fetch [shortname] # 从远程仓库抓取数据  </li>
<li>git merge  origin/serverfix # fetch之后合并</li>
<li>git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt;  #取回远程主机某个分支的更新，再与本地的指定分支合并。如果当是前分支，可省略本地分支  </li>
<li>git pull –rebase origin master #如果远程库中有些文件本地库没有，就会有冲突，所以在push之前先执行这句命令合并一下代码即可   </li>
<li>git remote rm origin #解除origin的绑定。 </li>
<li>git push [远程名] :[分支名] #删除远程分支  </li>
<li>git checkout –track origin/serverfix # 跟踪分支 同 <code>git checkout -b serverfix origin/serverfix</code></li>
</ul>
</li>
<li>branch管理：  <ul>
<li>git branch #查看当前分支  </li>
<li>git branch a # 新建分支a  </li>
<li>git checkout a #切换到a分支  </li>
<li>git merge a #当前分支与a分支合并  </li>
<li>git branch -d a #删除a分支  </li>
<li>git branch –no-merged #未合并的分支  </li>
<li>git branch –merged  </li>
<li>git branch -v  # 查看各个分支最后一个提交对象的信息  </li>
</ul>
</li>
<li>其它一些命令：  <ul>
<li>git status # 查看工作区、暂存区状态  </li>
<li>git log  # 查看历史提交信息 可以加上<code>--pretty=oneline</code>参数，然结果显示在一行。 </li>
<li>git diff a.txt #比较index和work directory中a.txt差异 </li>
<li>git reflog # 显示命令历史  </li>
<li>git reset –hard HEAD^ #回退到上一个版本  </li>
<li>git reset –hard HEAD~100 # 回退到上100个版本  </li>
<li>git reset –hard 4589 # 4589是版本号   </li>
<li>git checkout – a.txt #工作区做了修改没有add和commit可以丢弃修改  </li>
<li>git reset HEAD a.txt # 在add了之后，可以把暂存区的修改撤销掉（unstage），即，使上一条add命令失效，可以接着 git checkout – a.txt丢弃工作区的修改  </li>
<li>rm a.txt # 想删除工作区的文件，可以鼠标右键也可以用这条命令删除  </li>
<li>git rm a.txt # 删除暂存区和跟踪清单的文件  </li>
<li>git stash # 保存现场  </li>
<li>git stash pop # 恢复现场，或者<code>git stash apply</code> + <code>git stash drop</code>.<br><code>git diff [&lt;path&gt;...]</code>：这个命令最常用，在每次add进入index前会运行这个命令，查看即将add进入index时所做的内容修改，即<strong>working directory和index的差异</strong>。<br><code>git diff --cached [&lt;path&gt;...]</code>：这个命令初学者不太常用，却非常有用，它表示查看已经add进入index但是尚未commit的内容同最后一次commit时的内容的差异。即<strong>index和git directory的差异.</strong><br><code>git diff --cached [&lt;commit&gt;] [&lt;path&gt;...]</code>：这个命令初学者用的更少，也非常有用，它表示查看已经add进入index但是尚未commit的内容同指定的<code>&lt;commit&gt;</code>之间的差异，和上面一条很相似，差别仅仅<code>&lt;commit&gt;</code>，即<strong>index和git directory中指定版本的差异。</strong><br><code>git diff &lt;commit&gt; [&lt;path&gt;...]</code>：这个命令用来查看<strong>工作目录和指定<code>&lt;commit&gt;</code>的commit之间的差别</strong>，如果要和Git directory中最新版比较差别，则<code>&lt;commit&gt;</code>=HEAD。如果要和某一个branch比较差别，<code>&lt;commit&gt;</code>=分支名字。<br><code>git diff &lt;commit&gt; &lt;commit&gt; [&lt;path&gt;...]</code>：这个命令用来比较git directory中任意两个<code>&lt;commit&gt;</code>之间的差别，如果想比较任意一个<code>&lt;commit&gt;</code>和最新版的差别，把其中一个<code>&lt;commit&gt;</code>换成HEAD即可。<br>同时可以通过<code>[&lt;path&gt;...]</code>参数将比较限定于特点的目录或文件<br>一般我们总会有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表。通常都是些自动生成的文件，比如日志文件，或者编译过程中创建的临时文件等。我们可以创建一个名为 <code>.gitignore</code> 的文件，列出要忽略的文件模式。比如<code>.[oa]</code>是为.o或.a的文件忽略不管。  </li>
</ul>
</li>
</ul>
<h3 id="git-原理"><a href="#git-原理" class="headerlink" title="git 原理"></a>git 原理</h3><p><img src="/images/git0.png" alt="" title="每次保存文件快照"><br><img src="/images/git1.jpg" alt="" title="工作空间，暂存区，版本库之间的关系">    </p>
<hr>
<h3 id="refrence"><a href="#refrence" class="headerlink" title="refrence:"></a>refrence:</h3><p>diff:  <a href="http://blog.csdn.net/hudashi/article/details/7664385" target="_blank" rel="external">http://blog.csdn.net/hudashi/article/details/7664385</a><br><a href="http://stormzhang.com/github/2016/05/30/learn-github-from-zero3/" target="_blank" rel="external">http://stormzhang.com/github/2016/05/30/learn-github-from-zero3/</a><br><a href="http://git.oschina.net/progit" target="_blank" rel="external">http://git.oschina.net/progit</a><br><a href="http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="external">http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000</a><br>分支： <a href="http://www.open-open.com/lib/view/open1328069889514.html#articleHeader3" target="_blank" rel="external">http://www.open-open.com/lib/view/open1328069889514.html#articleHeader3</a><br><a href="http://www.ruanyifeng.com/blog/2014/06/git_remote.html" target="_blank" rel="external">http://www.ruanyifeng.com/blog/2014/06/git_remote.html</a></p>
<blockquote>
<p>Written with <a href="https://stackedit.io/" target="_blank" rel="external">StackEdit</a>&amp;sublime  </p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;别说话，就是干。 &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[x] 安装&lt;/li&gt;
&lt;li&gt;[x] 命令的学习&lt;/li&gt;
&lt;li&gt;[x] 与github关联push&lt;/li&gt;
&lt;li&gt;[x] git原理的学习
    
    </summary>
    
      <category term="技术" scheme="xiaodouhua.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="git" scheme="xiaodouhua.github.io/tags/git/"/>
    
      <category term="github" scheme="xiaodouhua.github.io/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>《精进》</title>
    <link href="xiaodouhua.github.io/2016/08/31/%E3%80%8A%E7%B2%BE%E8%BF%9B%E3%80%8B/"/>
    <id>xiaodouhua.github.io/2016/08/31/《精进》/</id>
    <published>2016-08-31T15:19:55.000Z</published>
    <updated>2016-09-01T01:15:19.802Z</updated>
    
    <content type="html"><![CDATA[<p>最近刚刚看完采铜的《精进》，顺便整理一下子书的内容。<br><a id="more"></a><br>全书分为<strong>时间，选择，行动，学习，思维，才能，成功</strong>七个方面。算是一本半鸡汤半实用类工具书。说了的许多东西其实更重要的是需要去实践和在生活中刻意的去训练一下的思维和做事的方法。</p>
<hr>
<h2 id="时间"><a href="#时间" class="headerlink" title="时间"></a>时间</h2><p><strong>时间之尺</strong>这一章是关于如何对待时间的。时间是相当宝贵的。而随着成长和各种琐事以及手机还有当下各种段子新闻的侵蚀。我逐渐感觉到了时间是非常非常不够用的。想到这，我又想到了知乎上有一个关于“<a href="https://www.zhihu.com/question/30824529" target="_blank" rel="external">随着年龄增长，为什么感到时间过得越来越快</a>”的问题，相关讨论记忆比较深刻的是小时候世界是新奇的，每天获得的东西非常多，信息量大，所以我们感到小时候非常漫长，18岁以前，尤其是小学和中学，日子十分缓慢。而现在接受世界的信息比较少，每天过着相同的生活。好奇心比较少。所以日子如流水飞过。  </p>
<blockquote>
<p>人类对世界的感知几乎都是对数的（Logarithmic）而非线性累加的（additive）。我们对时间的感知同样是这样：2~3岁经历了一年，和我已有生命的总长度一样；到了80岁，再经历一年，只是我经历岁月的1/80。“比例感知”让我们对时间的概念，对数级增加。<img src="https://pic3.zhimg.com/320b8eaf1cb3dd8be7b8280eba50e7b6_r.png" alt=""><br>显然这种感知是”非理性“的。对我们”好好过着一辈子“也是不利的。但是像”被宰困境“中一样，我们可以依靠”理性“来”纠正“这种”感知偏差“。但是这需要我们时时刻刻保持对时间的敏感。  </p>
<p>心理学家认为：我们对时间的感知同时和我们的经历有关—如果一件事对于我们来说是”激动人心“的，新奇的，未知的。这样的记忆在我们脑海中”感觉“时间更长，这样的经历几分钟，和一个下午捏泡泡的记忆相比，前者会占据更多的空间。这并不是你的大脑为这些记忆创造了更多的信息，而是制造了更多”拷贝“。对于年轻的你：几乎每一刻都是新奇的，令人激动的：第一次说话，第一次走路，第一次看到动画片，第一次学习数学，第一次去商场，第一次上学，第一次交到交心小伙伴。这些里程碑式的记忆在随后你的岁月中，会”感觉起来“占据了”更长的时间“ （Because your brain is putting in allthese new details, when you think back on it later, there’s so much more to remember, it just seems slower）。</p>
<p>作者：爱小臭<br>链接：<a href="https://www.zhihu.com/question/30824529/answer/74554726" target="_blank" rel="external">https://www.zhihu.com/question/30824529/answer/74554726</a><br>来源：知乎<br>著作权归作者所有，转载请联系作者获得授权。  </p>
</blockquote>
<p>所以想要人生过得长，一个好方法就是回归童年，多对世界充满一点好奇，多有一些人生的里程碑，适当（多？）尝试不同的东西，跳出自己的舒适区。这才是坠吼的。  </p>
<p>书中主要说了对待时间的许多侧面。列出来如下几点：</p>
<ul>
<li>向孩子去学习“郑重”的态度，对所做的事情多一点耐心和投入。</li>
<li>对待过去，应该用乐观主义视角去看待。毕竟，“What’s done is done”，不管好坏，接受之，我们是活在现在和未来的。过去的我已消失。而未来又是由现在决定的。因此，我们是要活在当下。视未来于当下。  </li>
<li>在工作中采用未来视角。在休闲中采用享乐主义视角。意思就是说工作时要为了完成未来的目标和任务，抛弃一些当下的享乐。而在休息时，就应该抛却工作和学习，enjoy life。在休息时也要找到自己的“独享时刻”，不疲于奔命。  </li>
<li>制定实现目标的计划，同时考虑到非计划因素。这会让我们生活更有序。  </li>
<li>未来可分为<strong>近期未来</strong>和<strong>远期未来</strong>，近期未来人们想的更多的是怎么去做。远期未来更偏抽象和战略化。拖延症的一个原因就是远期未来比较缥缈，近期的享受取而代之了。所以对远期未来来说，要“重战术，轻战略”。多去想想如何实施并且去实施它。对近期未来，我们往往可能去抄近路而去快速解决问题，放弃了充分锻炼的机会。近期未来要郑重。想想五年的规划。  </li>
<li>少做半衰期短的事情。我们往往会选择无能和执行无能。一条简单的法则：<strong>只要这件事的效益可以被累积，就去做</strong>。比如积累知识训练技能，构建新的思维模式，提升审美品位，保持和促进健康（做的太差了额），建立信任的关系，寻找和获得稀缺性的资源。多读经典也是其中之一。半衰期短的事情一个很典型的例子就是生活中的各种噪音，刷微博，朋友圈，知乎等。这些都是碎片化的无意义的信息。应该过滤掉。  </li>
<li>工作要快，生活要慢。减少被动时休闲的时间比例，多进行写创造型的休息。保持至少一项长期的业余爱好。朱光潜如是说： <blockquote>
<p>做学问，做事业，在人生中都只能算第二桩事。人生的第一桩事是生活，我所谓‘生活’是‘享受’，是‘领略’，是‘培养生机’，若为做学问而忘却生活，那种学问和事业在人生中便失去真正意义和价值。  </p>
</blockquote>
</li>
</ul>
<h2 id="选择"><a href="#选择" class="headerlink" title="选择"></a>选择</h2><p>说点虚的：</p>
<blockquote>
<p>人生任何一个阶段的“筛选”都只是一种形式，别被这些一时的标准迷惑。定义你最终归宿的，一定是你能力和欲望综合的那个真实的你。人生最重要的事情就是如何清楚地认识自己。我是谁，我的性格如何，我有什么优缺点，我适合做什么，我喜欢或不喜欢什么样的生活方式。最重要的是，这些问题的答案很复杂，不见得立刻就有答案，这些问题的答案还是不断变化的，这些问题需要你以自己和世界的样子互为对照，去不断地追问，又不断地在追问后塑造。</p>
<p>这需要漫长的时间，会发生很多反复，你会不认识自己，也会不断地重新认识自己。</p>
<p>世俗的选择永远都会给努力的人以入口，也永远都会给想离开的人以出口，只是你要付出相应的代价。那么认识自己这件事情，恐怕越早开始越好，因为越早，你就可以以越小的代价去选择你是应当离开，还是应当留下。</p>
<p>作者：skiptomylou<br>链接：<a href="https://www.zhihu.com/question/23819007/answer/107332874" target="_blank" rel="external">https://www.zhihu.com/question/23819007/answer/107332874</a><br>来源：知乎<br>著作权归作者所有，转载请联系作者获得授权。  </p>
</blockquote>
<p>另外一个回答：</p>
<blockquote>
<p>1、选择比努力重要。这是前半句。<br>后半句是：“选择”本身就是一种需要大量练习、大量努力来磨练的技能。</p>
<p>优秀的选择能力本身就是一个领导者的必备素质，也叫作“决策能力”，它包含了对大量信息的收集、分析和仔细对比，这是一个极为辛苦的认知过程。而它也包含了在信息不那么对称的情况下进行一把小小的赌博。如果把它总结成一句话：我们必须努力学会真正清晰地思考和决策方式，才能让自己是“做选择”，而不是“做赌博”。</p>
<p>2、人生最重要的是，是你内心深处的欲望，你要努力成为一个什么样的人。这是前半句，它告诉我们内在动机是最重要的。</p>
<p>后半句是：每一个人的自我发展过程中都需要足够大量的尝试，与外界互动，获得反馈和提升。社会环境成就人，内心真正的渴望只是一个指南针。能让你到达目的地的不是指南针，而是真正开船航海，学会搏击风浪，绕开暗礁。</p>
<p>作者：学霸猫<br>链接：<a href="https://www.zhihu.com/question/23819007/answer/108306964" target="_blank" rel="external">https://www.zhihu.com/question/23819007/answer/108306964</a><br>来源：知乎<br>著作权归作者要所有，转载请联系作者获得授权。 </p>
</blockquote>
<p>说点书中的具体操作方法：</p>
<ul>
<li>从终极问题出发，其实就是我们要成为什么样的人，格局放大一点。一个成熟的人，标准来自于自己的内心。要有自己的标准，不与环境妥协。问一问，最近一周（一月）做的最有意义的一件事是：<strong>___</strong> </li>
<li>逃离假设的牢笼，用更灵活的思维去选择。发现潜在的选项。形成更灵活的思维框架。（目标悬置，能力嫁接，特性改造）‘人生就是一个连点成线的过程’  </li>
<li>选择太多建立维度表去加权分析。得到最优的结果。  </li>
<li>人生是持续而反复的选择过程。不要因为预设规则而放弃个人追求。重新选择也不需要全部推倒重来（连点成线啊同志们）。  <h2 id="行动"><a href="#行动" class="headerlink" title="行动"></a>行动</h2><blockquote>
<p>知道了很多大道理，依然过不好这一生。  </p>
</blockquote>
</li>
</ul>
<p><strong>JUST DO IT!</strong>  </p>
<ul>
<li>即刻行动，现在就是最好的时机（也不绝对，有些复杂的事需要先规划，但这也是行动的一种）。不要想等到完全准备好的那一天，因为不可能会有‘完全好’的那一天。<strong>种一棵树最好的时机是十年前，其次是现在。</strong>尤其是生活中的小事，即刻处理掉会发现心理轻松了很多。  </li>
<li>精益创业（Lean Startup）中有一个概念叫‘最小化可行性产品’（minimum variable product,MVP）,所以我们可以找到自己的MVP，将每一次不管读书还是做事的经历形成一个MVP。 </li>
<li>在行动中，把批评当做一种信息对待，同时不断修正自己的‘产品’。  </li>
<li>工作的切换问题。工作的切换点应该是完成了一段不可中断的过程，比如某一章最难的一个小节要尽量一口气读完不然下次又得重新进入。另一方面就是先啃掉最难的一部分，问题就很好解决了。</li>
<li>同性质工作一起处理。比如写论文提纲同时就可以制作PPT文字部分。等到论文绘图部分就可以完成PPT的相关图片制作。  </li>
<li>三行而后思。完成工作后要去反思。比如进度，完成情况，学到了什么，还有什么需要改进和学习的，与预期结果比较如何，意义是什么等等。<h2 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h2></li>
</ul>
<h2 id="思维"><a href="#思维" class="headerlink" title="思维"></a>思维</h2><h2 id="才能"><a href="#才能" class="headerlink" title="才能"></a>才能</h2><h2 id="成功"><a href="#成功" class="headerlink" title="成功"></a>成功</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近刚刚看完采铜的《精进》，顺便整理一下子书的内容。&lt;br&gt;
    
    </summary>
    
      <category term="读书" scheme="xiaodouhua.github.io/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>数学之美</title>
    <link href="xiaodouhua.github.io/2016/07/27/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/"/>
    <id>xiaodouhua.github.io/2016/07/27/数学之美/</id>
    <published>2016-07-27T15:20:18.000Z</published>
    <updated>2016-10-12T19:16:54.377Z</updated>
    
    <content type="html"><![CDATA[<h3 id="文字与语言"><a href="#文字与语言" class="headerlink" title="文字与语言"></a>文字与语言</h3><ul>
<li>文字编码符合最短编码原理</li>
<li>书面文短，压缩了体积</li>
<li>词是有限的封闭集合，语言是无限的开放集合，有着许多例外  </li>
</ul>
<h3 id="统计概率模型"><a href="#统计概率模型" class="headerlink" title="统计概率模型"></a>统计概率模型</h3><p>程序设计语言是上下文无关文法，复杂度为长度的平方。语言是上下文有关文法，复杂度是长度的6次方。主要解决的是一个词语序列是否构成一个可理解的合理的句子。<br><a id="more"></a><br>统计概率模型利用的是一个句子出现的概率简化到马尔科夫模型  </p>
<ul>
<li>也可用高阶马尔科夫模型（一般为三元模型）。</li>
<li>当词语出现的次数过于少时，要注意零概率问题和平滑方法。利用古德-图灵估计公式，将看得到的所有概率和调低一点，剩下的给看不见的小概率。  </li>
<li>将其用于中文分词：将一个句子用几种分词方法分割为不同的序列，通过比较它们的概率，最大的就是最准确的。分词是一个动态规划问题。通过维特比（Viterbi）算法解决。  </li>
</ul>
<h3 id="隐马尔科夫模型-HMM"><a href="#隐马尔科夫模型-HMM" class="headerlink" title="隐马尔科夫模型(HMM)"></a>隐马尔科夫模型(HMM)</h3><p><a href="https://www.zhihu.com/question/20962240" target="_blank" rel="external">知乎</a>上有关于HMM的详细介绍。主要有三个问题：  </p>
<ul>
<li>根据已知模型计算产生序列的概率</li>
<li>根据产生的序列推测状态序列</li>
<li>给定足够数据量，估计HMM中的参数  </li>
</ul>
<p>主要算法有baum-welch algorithm ,viterbi algorithm, forward algorithm.   </p>
<hr>
<p>前七章看完了，后面的数学还是挺难得，尤其是看了HMM相关知识之后，慢慢看吧。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;文字与语言&quot;&gt;&lt;a href=&quot;#文字与语言&quot; class=&quot;headerlink&quot; title=&quot;文字与语言&quot;&gt;&lt;/a&gt;文字与语言&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;文字编码符合最短编码原理&lt;/li&gt;
&lt;li&gt;书面文短，压缩了体积&lt;/li&gt;
&lt;li&gt;词是有限的封闭集合，语言是无限的开放集合，有着许多例外  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;统计概率模型&quot;&gt;&lt;a href=&quot;#统计概率模型&quot; class=&quot;headerlink&quot; title=&quot;统计概率模型&quot;&gt;&lt;/a&gt;统计概率模型&lt;/h3&gt;&lt;p&gt;程序设计语言是上下文无关文法，复杂度为长度的平方。语言是上下文有关文法，复杂度是长度的6次方。主要解决的是一个词语序列是否构成一个可理解的合理的句子。&lt;br&gt;
    
    </summary>
    
      <category term="读书" scheme="xiaodouhua.github.io/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="算法" scheme="xiaodouhua.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="数学" scheme="xiaodouhua.github.io/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>Markdown语法</title>
    <link href="xiaodouhua.github.io/2016/05/17/Markdown%E8%AF%AD%E6%B3%95/"/>
    <id>xiaodouhua.github.io/2016/05/17/Markdown语法/</id>
    <published>2016-05-16T22:44:20.000Z</published>
    <updated>2016-10-12T17:26:31.056Z</updated>
    
    <content type="html"><![CDATA[<p>熟悉一下markdown。<a href="http://wowubuntu.com/markdown/#list" target="_blank" rel="external">官方文档</a>是这个。.<br>Markdown常用的语法规则</p>
<h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 一级标题</span><br><span class="line">## 二级标题</span><br><span class="line">### 三级标题</span><br></pre></td></tr></table></figure>
<a id="more"></a>  
<h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><p>无序列表加-或*，有序列表加1. 2. etc.  ,注意要有空格</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">* 1</span><br><span class="line">* 2</span><br><span class="line">* 3</span><br><span class="line">1. aaaa</span><br><span class="line">2. bbbb</span><br><span class="line">3. cccc</span><br></pre></td></tr></table></figure>
<ul>
<li>qaa<ul>
<li>sss</li>
<li>eee     </li>
</ul>
</li>
</ul>
<ol>
<li>aaaa</li>
<li>bbbb</li>
<li>cccc  </li>
</ol>
<h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><p>用<code>&gt;</code>表示，在一段话前面引用即可</p>
<blockquote>
<p>那美丽的天,总是一望无边<br>有粒种子埋在云下面<br>营养来自这满地污泥<br>生根发芽,仍然顺从天意<br>无数个雨点,在我面前洒满大地<br>站在这里,只有一个问题<br>向阳花!如果你只生长在黑暗下<br>向阳花,会不会害怕<br>———谢天笑《向阳花》</p>
</blockquote>
<h3 id="换行和段落"><a href="#换行和段落" class="headerlink" title="换行和段落"></a>换行和段落</h3><p><code>换行是两个空格加回车，段落是  空一行</code></p>
<h3 id="图片与链接"><a href="#图片与链接" class="headerlink" title="图片与链接"></a>图片与链接</h3><p>图片：</p>
<p><code>![加载失败](http://i3.buimg.com/25262b1d3a1d487a.jpg &quot;洱海&quot; )</code></p>
<p><img src="http://i3.buimg.com/25262b1d3a1d487a.jpg" alt="加载失败" title="洱海"></p>
<p>链接：<code>[stackedit](https://stackedit.io/editor)</code><a href="https://stackedit.io/editor" target="_blank" rel="external">stackedit</a></p>
<h3 id="粗体与斜体"><a href="#粗体与斜体" class="headerlink" title="粗体与斜体"></a>粗体与斜体</h3><p><code>**我是粗体字**</code> <code>*我是斜体字*</code><br><strong>我是粗体字</strong><br><em>我是斜体字</em></p>
<h3 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">| Tables        | Are           | Cool  |</span><br><span class="line">| ------------- |:-------------:| -----:|</span><br><span class="line">| col 3 is      | right-aligned | $1600 |</span><br><span class="line">| col 2 is      | centered      |   $12 |</span><br><span class="line">| zebra stripes | are neat      |    $1 |</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>Tables</th>
<th style="text-align:center">Are</th>
<th style="text-align:right">Cool</th>
</tr>
</thead>
<tbody>
<tr>
<td>col 3 is</td>
<td style="text-align:center">right-aligned</td>
<td style="text-align:right">$1600</td>
</tr>
<tr>
<td>col 2 is</td>
<td style="text-align:center">centered</td>
<td style="text-align:right">$12</td>
</tr>
<tr>
<td>zebra stripes</td>
<td style="text-align:center">are neat</td>
<td style="text-align:right">$1</td>
</tr>
</tbody>
</table>
<p>……好像不支持</p>
<h3 id="代码框"><a href="#代码框" class="headerlink" title="代码框"></a>代码框</h3><p>用三个`包围起来  </p>
<p>或者在段内用一对`包围起来也可以</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a=<span class="number">5</span></span><br><span class="line">b=<span class="number">8</span></span><br><span class="line"><span class="keyword">print</span> (a+b)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"hello world"</span>)</span><br><span class="line"><span class="keyword">while</span>(a):</span><br><span class="line">    print(<span class="string">"a is not zero!"</span>)</span><br><span class="line">    a--</span><br></pre></td></tr></table></figure>
<h3 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h3><p>用三个*即可</p>
<hr>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;熟悉一下markdown。&lt;a href=&quot;http://wowubuntu.com/markdown/#list&quot;&gt;官方文档&lt;/a&gt;是这个。.&lt;br&gt;Markdown常用的语法规则&lt;/p&gt;
&lt;h3 id=&quot;标题&quot;&gt;&lt;a href=&quot;#标题&quot; class=&quot;headerlink&quot; title=&quot;标题&quot;&gt;&lt;/a&gt;标题&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;# 一级标题&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;## 二级标题&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;### 三级标题&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="技术" scheme="xiaodouhua.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="markdown" scheme="xiaodouhua.github.io/tags/markdown/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="xiaodouhua.github.io/2016/05/05/Hello-World/"/>
    <id>xiaodouhua.github.io/2016/05/05/Hello-World/</id>
    <published>2016-05-05T02:10:17.000Z</published>
    <updated>2016-08-30T02:12:08.729Z</updated>
    
    <content type="html"><![CDATA[<p>Hello world!  </p>
<p>I’m coming!</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hello world!  &lt;/p&gt;
&lt;p&gt;I’m coming!&lt;/p&gt;

    
    </summary>
    
    
      <category term="随笔" scheme="xiaodouhua.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
</feed>
